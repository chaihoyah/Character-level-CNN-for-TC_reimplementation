{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CharacterConvNet.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1RClvNO9C7MIH-M92xN9S93ZbO4B7R_J9","authorship_tag":"ABX9TyOdV8EOI22fzR1K9jZQ4UMY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"2leEBC-mdkmK","executionInfo":{"status":"ok","timestamp":1622079543657,"user_tz":-540,"elapsed":458,"user":{"displayName":"Chaiho Shin","photoUrl":"","userId":"18301388949773723424"}}},"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","\n","class Net(nn.Module):\n","    def __init__(self, input_feature, input_length, num_classes, drop_prob, mode='small'):\n","        super(Net, self).__init__()\n","        np.random.seed(99)\n","        self.mode = mode\n","        if self.mode == 'small':\n","            l6_frame_length = int((input_length - 96)/27)\n","            self.conv = nn.Sequential(\n","                nn.Conv1d(input_feature, 256, 7, 1),\n","                nn.ReLU(),\n","                nn.MaxPool1d(3, 3),\n","\n","                nn.Conv1d(256, 256, 7, 1),\n","                nn.ReLU(),\n","                nn.MaxPool1d(3, 3),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","\n","                nn.Conv1d(256, 256, 3, 1),\n","                nn.ReLU(),\n","                nn.MaxPool1d(3, 3)\n","            )\n","\n","            self.fc = nn.Sequential(\n","                nn.Linear(l6_frame_length * 256, 1024),\n","                nn.Dropout(p=drop_prob),\n","                nn.Linear(1024, 1024),\n","                nn.Dropout(p=drop_prob),\n","                nn.Linear(1024, num_classes)\n","            )\n","        self.softmax = nn.Softmax(dim = 1)\n","\n","    def forward(self, x):\n","        out = self.conv(x)\n","        out = out.view(len(x), -1)\n","        out = self.fc(out)\n","        return self.softmax(out)\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qMRipRWid3EE","outputId":"47c3b3a6-3936-4163-ec14-f5a5f9564aa3"},"source":["import os\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import numpy as np\n","from nltk.corpus import wordnet\n","from collections import OrderedDict\n","import nltk\n","import pickle\n","import os\n","import copy\n","import re\n","\n","nltk.download('wordnet')\n","\n","data_addr = \"/content/drive/MyDrive/Data_CharacterConvNet\"\n","\n","alphabet_dic = {'a':0, 'b':1, 'c':2, 'd':3, 'e':4, 'f':5, 'g':6, 'h':7, 'i':8, 'j':9, 'k':10, 'l':11, 'm':12, 'n':13, 'o':14, 'p':15, 'q':16, 'r':17, 's':18, 't':19, 'u':20, 'v':21, 'w':22, 'x':23, 'y':24, 'z':25,\n"," '0':26, '1':27, '2':28, '3':29, '4':30, '5':31, '6':32, '7':33, '8':34, '9':35,\n"," \"-\":36, ',':37, ';':38, '.':39, '!':40, '?':41, ':':42, '\\'':43, '\"':44, '/':45, '\\\\':46, '|':47, '_':48, '@':49, '#':50, '$':51, '%':52, 'ˆ':53, '&':54, '*':55, '~':56, '`':57, '+':58, \"-\":59, '=':60, '<':61, '>':62, '(':63, ')':64, '[':65, ']':66, '{':67, '}':68, '\\n': 69}\n","\n","fullalphabet_dic = {'a':0, 'b':1, 'c':2, 'd':3, 'e':4, 'f':5, 'g':6, 'h':7, 'i':8, 'j':9, 'k':10, 'l':11, 'm':12, 'n':13, 'o':14, 'p':15, 'q':16, 'r':17, 's':18, 't':19, 'u':20, 'v':21, 'w':22, 'x':23, 'y':24, 'z':25,\n"," '0':26, '1':27, '2':28, '3':29, '4':30, '5':31, '6':32, '7':33, '8':34, '9':35,\n"," \"-\":36, ',':37, ';':38, '.':39, '!':40, '?':41, ':':42, '\\'':43, '\"':44, '/':45, '\\\\':46, '|':47, '_':48, '@':49, '#':50, '$':51, '%':52, 'ˆ':53, '&':54, '*':55, '~':56, '`':57, '+':58, \"-\":59, '=':60, '<':61, '>':62, '(':63, ')':64, '[':65, ']':66, '{':67, '}':68, '\\n': 69,\n"," 'A':70, 'B':71, 'C':72, 'D':73, 'E':74, 'F':75, 'G':76, 'H':77, 'I':78, 'J':79, 'K':80, 'L':81, 'M':82, 'N':83, 'O':84, 'P':85, 'Q':86, 'R':87, 'S':88, 'T':89, 'U':90, 'V':91, 'W':92, 'X':93, 'Y':94, 'Z':95}\n","alphabet_length = len(alphabet_dic)\n","print(alphabet_length)\n","char_length = 1014\n","\n","def get_data(path, col_num, augment_data, doLower):\n","    global data_addr\n","    global alphabet_dic\n","    global alphabet_length\n","    global char_length\n","\n","    def str_lower(str):\n","        return str.lower()\n","\n","    def to_str(num):\n","        return str(num)\n","\n","    def label_process(label):\n","        return label-1\n","\n","    full_addr = data_addr + path\n","    if not os.path.isfile(data_addr+path+'/pickles'+'train_basic.pickle'):\n","        if col_num == 3:\n","            train_csv = pd.read_csv(full_addr+'/train.csv', names=['label', 'title', 'data'])\n","            test_csv = pd.read_csv(full_addr+'/test.csv', names=['label', 'title', 'data'])\n","            train_csv['title'] = train_csv['title'].transform(to_str)\n","            test_csv['title'] = test_csv['title'].transform(to_str)\n","            train_csv['full data'] = train_csv[['title', 'data']].apply(''.join, axis=1)\n","            test_csv['full data'] = test_csv[['title', 'data']].apply(''.join, axis=1)\n","            if doLower:\n","                train_csv['full data'] = train_csv['full data'].transform(str_lower)\n","                test_csv['full data'] = test_csv['full data'].transform(str_lower)\n","            train_csv['label'] = train_csv['label'].transform(label_process)\n","            test_csv['label'] = test_csv['label'].transform(label_process)\n","            train_df = [[row['full data'], row['label']] for index,row in train_csv.iterrows()]\n","            test_df = [[row['full data'], row['label']] for index,row in test_csv.iterrows()]\n","        elif col_num == 4:\n","            train_csv = pd.read_csv(full_addr+'/train.csv', names=['label', 'title', 'question', 'answer'])\n","            test_csv = pd.read_csv(full_addr+'/test.csv', names=['label', 'title', 'question', 'answer'])\n","            print(test_csv['question'].head())\n","            print(test_csv['title'].head())\n","            print(test_csv['answer'].head())\n","            train_csv['title'] = train_csv['title'].transform(to_str)\n","            print(len(train_csv['answer']))\n","            train_csv['question'] = train_csv['question'].transform(to_str)\n","            train_csv['answer'] = train_csv['answer'].transform(to_str)\n","            test_csv['title'] = test_csv['title'].transform(to_str)\n","            test_csv['question'] = test_csv['question'].transform(to_str)\n","            test_csv['answer'] = test_csv['answer'].transform(to_str)\n","            train_csv['full data'] = train_csv[['title', 'question', 'answer']].apply(' '.join, axis=1)\n","            test_csv['full data'] = test_csv[['title', 'question', 'answer']].apply(' '.join, axis=1)\n","            print(train_csv['full data'])\n","            if doLower:\n","                train_csv['full data'] = train_csv['full data'].transform(str_lower)\n","                test_csv['full data'] = test_csv['full data'].transform(str_lower)\n","            train_csv['label'] = train_csv['label'].transform(label_process)\n","            test_csv['label'] = test_csv['label'].transform(label_process)\n","            print(train_csv['full data'])\n","            train_df = [[row['full data'], row['label']] for index,row in train_csv.iterrows()]\n","            test_df = [[row['full data'], row['label']] for index,row in test_csv.iterrows()]\n","            print(len(train_df))\n","        elif col_num == 2:\n","            train_csv = pd.read_csv(full_addr + '/train.csv', names=['label', 'data'])\n","            test_csv = pd.read_csv(full_addr + '/test.csv', names=['label', 'data'])\n","            if doLower:\n","                train_csv['data'] = train_csv['data'].transform(str_lower)\n","                test_csv['data'] = test_csv['data'].transform(str_lower)\n","            print(test_csv['data'].head())\n","            print(train_csv['label'])\n","            train_csv['label'] = train_csv['label'].transform(label_process)\n","            test_csv['label'] = test_csv['label'].transform(label_process)\n","            train_df = [[row['data'], row['label']] for index, row in train_csv.iterrows()]\n","            test_df = [[row['data'], row['label']] for index, row in test_csv.iterrows()]  \n","        np.random.shuffle(train_df)\n","        np.random.shuffle(test_df)\n","        with open(data_addr+path+'/pickles'+'train_basic.pickle', 'wb') as fw:\n","            pickle.dump(train_df, fw)\n","        with open(data_addr+path+'/pickles'+'test_basic.pickle', 'wb') as fa:\n","            pickle.dump(test_df, fa)\n","        \n","    else:\n","        with open(data_addr+path+'/pickles'+'train_basic.pickle', 'rb') as fw:\n","            train_df = pickle.load(fw)\n","        with open(data_addr+path+'/pickles'+'test_basic.pickle', 'rb') as fa:\n","            test_df = pickle.load(fa)\n","\n","    def str_cleanup(str):\n","        return str.strip().split()\n","\n","    def find_synonyms(word):\n","        syn = list()\n","        for synset in wordnet.synsets(word):\n","            for syn_word in synset.lemma_names():\n","                syn.append(syn_word)\n","        return list(OrderedDict.fromkeys(syn))\n","\n","    if augment_data == True:\n","        if not os.path.isfile(data_addr + path + '/pickles' + 'train_augmented.pickle'):\n","            vocab_set = set()\n","            train_augmented_df = copy.deepcopy(train_df)\n","            for idx_out, (data, label) in enumerate(train_df):\n","                data = str_cleanup(data)\n","                syn_list = list()\n","                replaceable_len = 0\n","                for idx_in,word in enumerate(data):\n","                    tmp = find_synonyms(word)\n","                    if len(tmp) > 0:\n","                        replaceable_len += 1\n","                        syn_list.append([idx_in, tmp])\n","                replace_num = np.random.geometric(p=0.5)\n","                if replaceable_len >0 and replace_num > 0:\n","                    replace_list = np.random.choice(replaceable_len, replace_num)\n","                    data = np.array(data)\n","                    for num in replace_list:\n","                        replace_word_len = len(syn_list[num][1])\n","                        replace_word_num = np.random.geometric(p=0.5)\n","                        if len(syn_list[num][1]) > replace_word_len:\n","                            data[syn_list[num][0]] = syn_list[num][1][replace_word_num]\n","                        else:\n","                            data[syn_list[num][0]] = syn_list[num][1][-1]\n","                    train_augmented_df.append([' '.join(data), label])\n","            with open(data_addr+path+'/pickles'+'train_augmented.pickle', 'wb') as fw:\n","                pickle.dump(train_augmented_df, fw)\n","        else:\n","            with open(data_addr+path+'/pickles'+'train_augmented.pickle', 'rb') as fw:\n","                train_augmented_df = pickle.load(fw)\n","        return train_augmented_df, test_df\n","    return train_df, test_df\n","\n","\n","# Press the green button in the gutter to run the script.\n","def onehot_encode(batch):\n","    out = torch.zeros(len(batch), alphabet_length, char_length) # (128, 69, 1014)?\n","    out_label = []\n","    for idx,item in enumerate(batch):\n","        out_label.append(item[1])\n","        for idx_in, char in enumerate(item[0][:-1015:-1]):\n","            try:\n","                out[idx][alphabet_dic[char]][idx_in] = 1\n","            except KeyError:\n","                continue\n","    return torch.Tensor(out), torch.LongTensor(out_label)\n","\n","def weights_init(m):\n","    if type(m) == nn.Conv1d or type(m) == nn.Linear:\n","        nn.init.normal_(m.weight, mean=0, std=0.05)\n","\n","def save_checkpoint(epoch, model, opt, path):\n","    state = {\n","        'Epoch': epoch,\n","        'State_dict': model.state_dict(),\n","        'optimizer': opt.state_dict()\n","    }\n","    torch.save(state, path)\n","\n","# ag, amazon, dbpedia, sogou -> 3 cols, label, title, data\n","# yahoo -> 4 cols, label, title, question, answer\n","# yelp -> 2 cols, label, data\n","# ag\n","net = Net(alphabet_length, char_length, 2, 0.5, 'small').cuda()\n","#net.apply(weights_init)\n","#checkpoint = torch.load(data_addr+'/model'+ '/amazon_review_polarity_basic_8epoch')\n","#net.load_state_dict(checkpoint['State_dict'])\n","dev = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('current device: ', dev)\n","net.to(dev)\n","\n","num_epochs = 31\n","batch_size = 128\n","loss_func = nn.CrossEntropyLoss()\n","lr = 1e-2\n","optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n","#optimizer.load_state_dict(checkpoint['optimizer'])\n","torch.manual_seed(99)\n","running_loss = 0\n","correct = 0\n","total = 0\n","\n","if __name__ == '__main__':\n","    train_data, test_data = get_data(\"/amazon_review_polarity_csv\", 3, False, True)\n","    one = int(len(train_data)/2)\n","    two = int(len(train_data)/2)*2\n","    #three = int(len(train_data)/10)*3\n","    #four = int(len(train_data)/10)*4\n","    #five = int(len(train_data)/10)*5\n","    #six = int(len(train_data)/10)*6\n","    #seven = int(len(train_data)/10)*7\n","    #eight = int(len(train_data)/10)*8\n","    #nine = int(len(train_data)/10)*9\n","    trainloader_list = []\n","    trainloader_list.append(torch.utils.data.DataLoader(train_data[:one], batch_size=batch_size, shuffle=True, collate_fn = onehot_encode))\n","    trainloader_list.append(torch.utils.data.DataLoader(train_data[one:], batch_size=batch_size, shuffle=True, collate_fn = onehot_encode))\n","    #trainloader_list.append(torch.utils.data.DataLoader(train_data[two:three], batch_size=batch_size, shuffle=True, collate_fn = onehot_encode))\n","    #trainloader_list.append(torch.utils.data.DataLoader(train_data[three:four], batch_size=batch_size, shuffle=True, collate_fn = onehot_encode))\n","    #trainloader_list.append(torch.utils.data.DataLoader(train_data[four:five], batch_size=batch_size, shuffle=True, collate_fn = onehot_encode))\n","    #trainloader_list.append(torch.utils.data.DataLoader(train_data[five:six], batch_size=batch_size, shuffle=True, collate_fn = onehot_encode))\n","    #trainloader_list.append(torch.utils.data.DataLoader(train_data[six:seven], batch_size=batch_size, shuffle=True, collate_fn = onehot_encode))\n","    #trainloader_list.append(torch.utils.data.DataLoader(train_data[seven:eight], batch_size=batch_size, shuffle=True, collate_fn = onehot_encode))\n","    #trainloader_list.append(torch.utils.data.DataLoader(train_data[eight:nine], batch_size=batch_size, shuffle=True, collate_fn = onehot_encode))\n","    #trainloader_list.append(torch.utils.data.DataLoader(train_data[nine:], batch_size=batch_size, shuffle=True, collate_fn = onehot_encode))\n","    testloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn = onehot_encode)\n","\n","    for epoch in range(0, num_epochs):\n","        print(\"epoch: \", epoch)\n","        if epoch != 0 and epoch%3 == 0:\n","            lr *= 0.5\n","            optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n","\n","        running_loss = 0\n","        correct = 0\n","        total = 0\n","        net.train()\n","        trainloader = trainloader_list[epoch % 2]\n","            \n","        for idx, (x, y) in enumerate(iter(trainloader), 0):\n","            x, y = x.to(dev), y.to(dev)\n","            out = net(x)\n","            loss = loss_func(out, y)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            running_loss += loss.item()\n","            if idx % 50 == 49:\n","                correct = (torch.max(out, 1)[1] == y).sum().item()\n","                total = batch_size\n","                print('Training Accuracy: %d ' % (100.0 * correct / total))\n","                print('%d/%d' % (correct, total))\n","                print('RunningLoss %5d: %.3f' % (idx + 1, running_loss))\n","                running_loss = 0\n","\n","        correct_eachbatch = 0\n","        total_eachbatch = 0\n","        correct = 0\n","        total = 0\n","        net.eval()\n","        if epoch == 10 or epoch == 21 or epoch == 31:\n","            print(\"epoch: \", epoch)\n","            with torch.no_grad():\n","                for (x, y) in iter(testloader):\n","                    x, y = x.to(dev), y.to(dev)\n","                    out = net.forward(x)\n","                    predicted = torch.max(out, 1)[1]\n","                    loss = loss_func(out, y)\n","                    print(\"test loss: \", loss.item())\n","                    correct_eachbatch += (predicted == y).sum().item()\n","                    correct += correct_eachbatch\n","                    print(len(y))\n","                    total_eachbatch += len(y)\n","                    print(total_eachbatch)\n","                    total += total_eachbatch\n","                    print('Accuracy for test_batch: %.3f %%' % (100.0 * correct_eachbatch / total_eachbatch))\n","                    print('--------------------------------')\n","                    correct_eachbatch = 0\n","                    total_eachbatch = 0\n","            print('Accuracy for test: %.3f %%' % (100.0 * correct / total))\n","            print('--------------------------------')\n","    correct_eachbatch = 0\n","    total_eachbatch = 0\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for (x, y) in iter(testloader):\n","            x, y = x.to(dev), y.to(dev)\n","            out = net.forward(x)\n","            predicted = torch.max(out, 1)[1]\n","            loss = loss_func(out, y)\n","            print(\"test loss: \", loss.item())\n","            correct_eachbatch += (predicted == y).sum().item()\n","            correct += correct_eachbatch\n","            print(len(y))\n","            total_eachbatch += len(y)\n","            print(total_eachbatch)\n","            total += total_eachbatch\n","            print('Accuracy for test_batch: %.3f %%' % (100.0 * correct_eachbatch / total_eachbatch))\n","            print('--------------------------------')\n","            correct_eachbatch = 0\n","            total_eachbatch = 0\n","        \n","    print('Accuracy for test: %.3f %%' % (100.0 * correct / total))\n","    print('--------------------------------')\n","    save_checkpoint(num_epochs, net, optimizer, data_addr+'/model'+ '/amazon_review_full_TH_31epoch')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","69\n","current device:  cuda:0\n","epoch:  0\n","Training Accuracy: 51 \n","66/128\n","RunningLoss    50: 34.658\n","Training Accuracy: 50 \n","64/128\n","RunningLoss   100: 34.668\n","Training Accuracy: 42 \n","55/128\n","RunningLoss   150: 34.665\n","Training Accuracy: 53 \n","69/128\n","RunningLoss   200: 34.656\n","Training Accuracy: 47 \n","61/128\n","RunningLoss   250: 34.655\n","Training Accuracy: 48 \n","62/128\n","RunningLoss   300: 34.654\n","Training Accuracy: 52 \n","67/128\n","RunningLoss   350: 34.663\n","Training Accuracy: 57 \n","73/128\n","RunningLoss   400: 34.663\n","Training Accuracy: 53 \n","69/128\n","RunningLoss   450: 34.653\n","Training Accuracy: 54 \n","70/128\n","RunningLoss   500: 34.663\n","Training Accuracy: 42 \n","55/128\n","RunningLoss   550: 34.661\n","Training Accuracy: 48 \n","62/128\n","RunningLoss   600: 34.661\n","Training Accuracy: 57 \n","74/128\n","RunningLoss   650: 34.658\n","Training Accuracy: 50 \n","65/128\n","RunningLoss   700: 34.656\n","Training Accuracy: 52 \n","67/128\n","RunningLoss   750: 34.661\n","Training Accuracy: 54 \n","70/128\n","RunningLoss   800: 34.662\n","Training Accuracy: 47 \n","61/128\n","RunningLoss   850: 34.665\n","Training Accuracy: 48 \n","62/128\n","RunningLoss   900: 34.663\n","Training Accuracy: 47 \n","61/128\n","RunningLoss   950: 34.667\n","Training Accuracy: 54 \n","70/128\n","RunningLoss  1000: 34.660\n","Training Accuracy: 51 \n","66/128\n","RunningLoss  1050: 34.658\n","Training Accuracy: 45 \n","58/128\n","RunningLoss  1100: 34.659\n","Training Accuracy: 53 \n","68/128\n","RunningLoss  1150: 34.652\n","Training Accuracy: 48 \n","62/128\n","RunningLoss  1200: 34.661\n","Training Accuracy: 48 \n","62/128\n","RunningLoss  1250: 34.669\n","Training Accuracy: 53 \n","68/128\n","RunningLoss  1300: 34.656\n","Training Accuracy: 50 \n","65/128\n","RunningLoss  1350: 34.660\n","Training Accuracy: 49 \n","63/128\n","RunningLoss  1400: 34.659\n","Training Accuracy: 53 \n","69/128\n","RunningLoss  1450: 34.662\n","Training Accuracy: 44 \n","57/128\n","RunningLoss  1500: 34.661\n","Training Accuracy: 50 \n","64/128\n","RunningLoss  1550: 34.659\n","Training Accuracy: 44 \n","57/128\n","RunningLoss  1600: 34.655\n","Training Accuracy: 54 \n","70/128\n","RunningLoss  1650: 34.652\n","Training Accuracy: 46 \n","60/128\n","RunningLoss  1700: 34.665\n","Training Accuracy: 52 \n","67/128\n","RunningLoss  1750: 34.665\n","Training Accuracy: 49 \n","63/128\n","RunningLoss  1800: 34.659\n","Training Accuracy: 51 \n","66/128\n","RunningLoss  1850: 34.655\n","Training Accuracy: 51 \n","66/128\n","RunningLoss  1900: 34.658\n","Training Accuracy: 57 \n","73/128\n","RunningLoss  1950: 34.665\n","Training Accuracy: 50 \n","65/128\n","RunningLoss  2000: 34.657\n","Training Accuracy: 47 \n","61/128\n","RunningLoss  2050: 34.661\n","Training Accuracy: 48 \n","62/128\n","RunningLoss  2100: 34.643\n","Training Accuracy: 53 \n","68/128\n","RunningLoss  2150: 34.665\n","Training Accuracy: 57 \n","73/128\n","RunningLoss  2200: 34.657\n","Training Accuracy: 52 \n","67/128\n","RunningLoss  2250: 34.661\n","Training Accuracy: 51 \n","66/128\n","RunningLoss  2300: 34.656\n","Training Accuracy: 48 \n","62/128\n","RunningLoss  2350: 34.656\n","Training Accuracy: 47 \n","61/128\n","RunningLoss  2400: 34.670\n","Training Accuracy: 53 \n","68/128\n","RunningLoss  2450: 34.661\n","Training Accuracy: 46 \n","60/128\n","RunningLoss  2500: 34.662\n","Training Accuracy: 44 \n","57/128\n","RunningLoss  2550: 34.659\n","Training Accuracy: 57 \n","73/128\n","RunningLoss  2600: 34.647\n","Training Accuracy: 53 \n","68/128\n","RunningLoss  2650: 34.669\n","Training Accuracy: 46 \n","60/128\n","RunningLoss  2700: 34.657\n","Training Accuracy: 50 \n","64/128\n","RunningLoss  2750: 34.636\n","Training Accuracy: 44 \n","57/128\n","RunningLoss  2800: 34.661\n","Training Accuracy: 50 \n","65/128\n","RunningLoss  2850: 34.661\n","Training Accuracy: 49 \n","63/128\n","RunningLoss  2900: 34.660\n","Training Accuracy: 57 \n","73/128\n","RunningLoss  2950: 34.645\n","Training Accuracy: 52 \n","67/128\n","RunningLoss  3000: 34.649\n","Training Accuracy: 41 \n","53/128\n","RunningLoss  3050: 34.659\n","Training Accuracy: 51 \n","66/128\n","RunningLoss  3100: 34.643\n","Training Accuracy: 44 \n","57/128\n","RunningLoss  3150: 34.664\n","Training Accuracy: 48 \n","62/128\n","RunningLoss  3200: 34.660\n","Training Accuracy: 41 \n","53/128\n","RunningLoss  3250: 34.661\n","Training Accuracy: 52 \n","67/128\n","RunningLoss  3300: 34.656\n","Training Accuracy: 55 \n","71/128\n","RunningLoss  3350: 34.650\n","Training Accuracy: 45 \n","58/128\n","RunningLoss  3400: 34.645\n","Training Accuracy: 57 \n","73/128\n","RunningLoss  3450: 34.647\n","Training Accuracy: 50 \n","65/128\n","RunningLoss  3500: 34.657\n","Training Accuracy: 52 \n","67/128\n","RunningLoss  3550: 34.637\n","Training Accuracy: 45 \n","58/128\n","RunningLoss  3600: 34.655\n","Training Accuracy: 48 \n","62/128\n","RunningLoss  3650: 34.645\n","Training Accuracy: 50 \n","64/128\n","RunningLoss  3700: 34.639\n","Training Accuracy: 53 \n","68/128\n","RunningLoss  3750: 34.640\n","Training Accuracy: 48 \n","62/128\n","RunningLoss  3800: 34.632\n","Training Accuracy: 47 \n","61/128\n","RunningLoss  3850: 34.634\n","Training Accuracy: 54 \n","70/128\n","RunningLoss  3900: 34.624\n","Training Accuracy: 52 \n","67/128\n","RunningLoss  3950: 34.632\n","Training Accuracy: 56 \n","72/128\n","RunningLoss  4000: 34.624\n","Training Accuracy: 50 \n","64/128\n","RunningLoss  4050: 34.629\n","Training Accuracy: 61 \n","79/128\n","RunningLoss  4100: 34.586\n","Training Accuracy: 55 \n","71/128\n","RunningLoss  4150: 34.607\n","Training Accuracy: 51 \n","66/128\n","RunningLoss  4200: 34.633\n","Training Accuracy: 45 \n","58/128\n","RunningLoss  4250: 34.576\n","Training Accuracy: 53 \n","69/128\n","RunningLoss  4300: 34.599\n","Training Accuracy: 56 \n","72/128\n","RunningLoss  4350: 34.584\n","Training Accuracy: 46 \n","60/128\n","RunningLoss  4400: 34.525\n","Training Accuracy: 53 \n","68/128\n","RunningLoss  4450: 34.575\n","Training Accuracy: 48 \n","62/128\n","RunningLoss  4500: 34.519\n","Training Accuracy: 50 \n","65/128\n","RunningLoss  4550: 34.505\n","Training Accuracy: 50 \n","64/128\n","RunningLoss  4600: 34.607\n","Training Accuracy: 61 \n","79/128\n","RunningLoss  4650: 34.424\n","Training Accuracy: 50 \n","64/128\n","RunningLoss  4700: 34.495\n","Training Accuracy: 45 \n","58/128\n","RunningLoss  4750: 34.504\n","Training Accuracy: 52 \n","67/128\n","RunningLoss  4800: 34.477\n","Training Accuracy: 55 \n","71/128\n","RunningLoss  4850: 34.453\n","Training Accuracy: 53 \n","68/128\n","RunningLoss  4900: 34.456\n","Training Accuracy: 55 \n","71/128\n","RunningLoss  4950: 34.483\n","Training Accuracy: 42 \n","54/128\n","RunningLoss  5000: 34.510\n","Training Accuracy: 50 \n","65/128\n","RunningLoss  5050: 34.499\n","Training Accuracy: 54 \n","70/128\n","RunningLoss  5100: 34.496\n","Training Accuracy: 60 \n","78/128\n","RunningLoss  5150: 34.526\n","Training Accuracy: 52 \n","67/128\n","RunningLoss  5200: 34.490\n","Training Accuracy: 50 \n","65/128\n","RunningLoss  5250: 34.597\n","Training Accuracy: 53 \n","69/128\n","RunningLoss  5300: 34.533\n","Training Accuracy: 53 \n","68/128\n","RunningLoss  5350: 34.495\n","Training Accuracy: 47 \n","61/128\n","RunningLoss  5400: 34.494\n","Training Accuracy: 52 \n","67/128\n","RunningLoss  5450: 34.421\n","Training Accuracy: 61 \n","79/128\n","RunningLoss  5500: 34.564\n","Training Accuracy: 40 \n","52/128\n","RunningLoss  5550: 34.432\n","Training Accuracy: 60 \n","77/128\n","RunningLoss  5600: 34.518\n","Training Accuracy: 50 \n","65/128\n","RunningLoss  5650: 34.496\n","Training Accuracy: 54 \n","70/128\n","RunningLoss  5700: 34.510\n","Training Accuracy: 51 \n","66/128\n","RunningLoss  5750: 34.430\n","Training Accuracy: 59 \n","76/128\n","RunningLoss  5800: 34.451\n","Training Accuracy: 54 \n","70/128\n","RunningLoss  5850: 34.356\n","Training Accuracy: 46 \n","60/128\n","RunningLoss  5900: 34.420\n","Training Accuracy: 46 \n","59/128\n","RunningLoss  5950: 34.516\n","Training Accuracy: 54 \n","70/128\n","RunningLoss  6000: 34.487\n","Training Accuracy: 54 \n","70/128\n","RunningLoss  6050: 34.468\n","Training Accuracy: 45 \n","58/128\n","RunningLoss  6100: 34.468\n","Training Accuracy: 61 \n","79/128\n","RunningLoss  6150: 34.502\n","Training Accuracy: 48 \n","62/128\n","RunningLoss  6200: 34.393\n","Training Accuracy: 55 \n","71/128\n","RunningLoss  6250: 34.473\n","Training Accuracy: 47 \n","61/128\n","RunningLoss  6300: 34.431\n","Training Accuracy: 49 \n","63/128\n","RunningLoss  6350: 34.425\n","Training Accuracy: 57 \n","73/128\n","RunningLoss  6400: 34.456\n","Training Accuracy: 51 \n","66/128\n","RunningLoss  6450: 34.373\n","Training Accuracy: 54 \n","70/128\n","RunningLoss  6500: 34.362\n","Training Accuracy: 53 \n","69/128\n","RunningLoss  6550: 34.245\n","Training Accuracy: 50 \n","64/128\n","RunningLoss  6600: 34.231\n","Training Accuracy: 54 \n","70/128\n","RunningLoss  6650: 34.199\n","Training Accuracy: 56 \n","72/128\n","RunningLoss  6700: 34.078\n","Training Accuracy: 64 \n","82/128\n","RunningLoss  6750: 33.797\n","Training Accuracy: 55 \n","71/128\n","RunningLoss  6800: 33.458\n","Training Accuracy: 53 \n","68/128\n","RunningLoss  6850: 33.424\n","Training Accuracy: 64 \n","82/128\n","RunningLoss  6900: 33.576\n","Training Accuracy: 60 \n","78/128\n","RunningLoss  6950: 32.343\n","Training Accuracy: 50 \n","64/128\n","RunningLoss  7000: 34.451\n","Training Accuracy: 40 \n","52/128\n","RunningLoss  7050: 34.701\n","Training Accuracy: 46 \n","59/128\n","RunningLoss  7100: 34.672\n","Training Accuracy: 51 \n","66/128\n","RunningLoss  7150: 34.670\n","Training Accuracy: 51 \n","66/128\n","RunningLoss  7200: 34.657\n","Training Accuracy: 58 \n","75/128\n","RunningLoss  7250: 34.668\n","Training Accuracy: 53 \n","69/128\n","RunningLoss  7300: 34.663\n","Training Accuracy: 44 \n","57/128\n","RunningLoss  7350: 34.675\n","Training Accuracy: 53 \n","69/128\n","RunningLoss  7400: 34.666\n","Training Accuracy: 49 \n","63/128\n","RunningLoss  7450: 34.654\n","Training Accuracy: 50 \n","65/128\n","RunningLoss  7500: 34.614\n","Training Accuracy: 62 \n","80/128\n","RunningLoss  7550: 33.265\n","Training Accuracy: 60 \n","78/128\n","RunningLoss  7600: 32.187\n","Training Accuracy: 72 \n","93/128\n","RunningLoss  7650: 31.676\n","Training Accuracy: 62 \n","80/128\n","RunningLoss  7700: 30.663\n","Training Accuracy: 63 \n","81/128\n","RunningLoss  7750: 31.613\n","Training Accuracy: 68 \n","88/128\n","RunningLoss  7800: 30.035\n","Training Accuracy: 68 \n","88/128\n","RunningLoss  7850: 30.309\n","Training Accuracy: 72 \n","93/128\n","RunningLoss  7900: 30.536\n","Training Accuracy: 67 \n","86/128\n","RunningLoss  7950: 29.414\n","Training Accuracy: 65 \n","84/128\n","RunningLoss  8000: 30.298\n","Training Accuracy: 75 \n","97/128\n","RunningLoss  8050: 29.176\n","Training Accuracy: 65 \n","84/128\n","RunningLoss  8100: 28.321\n","Training Accuracy: 76 \n","98/128\n","RunningLoss  8150: 28.016\n","Training Accuracy: 71 \n","91/128\n","RunningLoss  8200: 28.145\n","Training Accuracy: 79 \n","102/128\n","RunningLoss  8250: 27.289\n","Training Accuracy: 67 \n","86/128\n","RunningLoss  8300: 27.794\n","Training Accuracy: 67 \n","86/128\n","RunningLoss  8350: 27.719\n","Training Accuracy: 70 \n","90/128\n","RunningLoss  8400: 26.293\n","Training Accuracy: 78 \n","100/128\n","RunningLoss  8450: 25.975\n","Training Accuracy: 78 \n","101/128\n","RunningLoss  8500: 27.654\n","Training Accuracy: 71 \n","92/128\n","RunningLoss  8550: 26.769\n","Training Accuracy: 79 \n","102/128\n","RunningLoss  8600: 26.890\n","Training Accuracy: 81 \n","104/128\n","RunningLoss  8650: 25.761\n","Training Accuracy: 80 \n","103/128\n","RunningLoss  8700: 25.246\n","Training Accuracy: 82 \n","105/128\n","RunningLoss  8750: 26.382\n","Training Accuracy: 76 \n","98/128\n","RunningLoss  8800: 25.470\n","Training Accuracy: 77 \n","99/128\n","RunningLoss  8850: 25.702\n","Training Accuracy: 76 \n","98/128\n","RunningLoss  8900: 25.949\n","Training Accuracy: 76 \n","98/128\n","RunningLoss  8950: 25.702\n","Training Accuracy: 75 \n","96/128\n","RunningLoss  9000: 25.907\n","Training Accuracy: 82 \n","105/128\n","RunningLoss  9050: 24.742\n","Training Accuracy: 84 \n","108/128\n","RunningLoss  9100: 24.586\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  9150: 25.957\n","Training Accuracy: 78 \n","100/128\n","RunningLoss  9200: 26.344\n","Training Accuracy: 77 \n","99/128\n","RunningLoss  9250: 24.669\n","Training Accuracy: 71 \n","92/128\n","RunningLoss  9300: 24.230\n","Training Accuracy: 76 \n","98/128\n","RunningLoss  9350: 24.678\n","Training Accuracy: 78 \n","101/128\n","RunningLoss  9400: 24.616\n","Training Accuracy: 84 \n","108/128\n","RunningLoss  9450: 25.719\n","Training Accuracy: 77 \n","99/128\n","RunningLoss  9500: 25.775\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  9550: 25.138\n","Training Accuracy: 85 \n","109/128\n","RunningLoss  9600: 23.874\n","Training Accuracy: 75 \n","97/128\n","RunningLoss  9650: 24.442\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  9700: 24.779\n","Training Accuracy: 82 \n","105/128\n","RunningLoss  9750: 24.836\n","Training Accuracy: 78 \n","101/128\n","RunningLoss  9800: 24.416\n","Training Accuracy: 79 \n","102/128\n","RunningLoss  9850: 24.603\n","Training Accuracy: 82 \n","106/128\n","RunningLoss  9900: 23.708\n","Training Accuracy: 78 \n","101/128\n","RunningLoss  9950: 24.315\n","Training Accuracy: 80 \n","103/128\n","RunningLoss 10000: 24.061\n","Training Accuracy: 76 \n","98/128\n","RunningLoss 10050: 23.876\n","Training Accuracy: 78 \n","100/128\n","RunningLoss 10100: 23.993\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 10150: 24.872\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 10200: 24.052\n","Training Accuracy: 79 \n","102/128\n","RunningLoss 10250: 24.793\n","Training Accuracy: 82 \n","106/128\n","RunningLoss 10300: 24.238\n","Training Accuracy: 82 \n","105/128\n","RunningLoss 10350: 23.694\n","Training Accuracy: 83 \n","107/128\n","RunningLoss 10400: 23.511\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 10450: 23.347\n","Training Accuracy: 79 \n","102/128\n","RunningLoss 10500: 23.304\n","Training Accuracy: 78 \n","100/128\n","RunningLoss 10550: 23.838\n","Training Accuracy: 83 \n","107/128\n","RunningLoss 10600: 23.837\n","Training Accuracy: 76 \n","98/128\n","RunningLoss 10650: 23.654\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 10700: 23.939\n","Training Accuracy: 79 \n","102/128\n","RunningLoss 10750: 23.255\n","Training Accuracy: 85 \n","110/128\n","RunningLoss 10800: 22.650\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 10850: 23.379\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 10900: 23.218\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 10950: 23.157\n","Training Accuracy: 68 \n","88/128\n","RunningLoss 11000: 23.821\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 11050: 24.710\n","Training Accuracy: 83 \n","107/128\n","RunningLoss 11100: 23.117\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 11150: 23.129\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 11200: 22.636\n","Training Accuracy: 82 \n","105/128\n","RunningLoss 11250: 22.658\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 11300: 23.262\n","Training Accuracy: 83 \n","107/128\n","RunningLoss 11350: 22.986\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 11400: 22.620\n","Training Accuracy: 85 \n","109/128\n","RunningLoss 11450: 23.431\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 11500: 22.873\n","Training Accuracy: 83 \n","107/128\n","RunningLoss 11550: 23.383\n","Training Accuracy: 74 \n","95/128\n","RunningLoss 11600: 22.796\n","Training Accuracy: 85 \n","110/128\n","RunningLoss 11650: 23.234\n","Training Accuracy: 80 \n","103/128\n","RunningLoss 11700: 22.977\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 11750: 22.726\n","Training Accuracy: 79 \n","102/128\n","RunningLoss 11800: 22.276\n","Training Accuracy: 81 \n","104/128\n","RunningLoss 11850: 22.380\n","Training Accuracy: 85 \n","110/128\n","RunningLoss 11900: 22.795\n","Training Accuracy: 82 \n","105/128\n","RunningLoss 11950: 23.211\n","Training Accuracy: 82 \n","105/128\n","RunningLoss 12000: 23.699\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 12050: 22.786\n","Training Accuracy: 82 \n","105/128\n","RunningLoss 12100: 22.482\n","Training Accuracy: 82 \n","105/128\n","RunningLoss 12150: 23.771\n","Training Accuracy: 80 \n","103/128\n","RunningLoss 12200: 23.609\n","Training Accuracy: 82 \n","106/128\n","RunningLoss 12250: 22.643\n","Training Accuracy: 79 \n","102/128\n","RunningLoss 12300: 23.133\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 12350: 22.525\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 12400: 22.778\n","Training Accuracy: 82 \n","105/128\n","RunningLoss 12450: 23.350\n","Training Accuracy: 85 \n","109/128\n","RunningLoss 12500: 23.038\n","Training Accuracy: 85 \n","109/128\n","RunningLoss 12550: 22.541\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 12600: 22.205\n","Training Accuracy: 82 \n","105/128\n","RunningLoss 12650: 22.753\n","Training Accuracy: 84 \n","108/128\n","RunningLoss 12700: 22.897\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 12750: 22.621\n","Training Accuracy: 84 \n","108/128\n","RunningLoss 12800: 22.434\n","Training Accuracy: 84 \n","108/128\n","RunningLoss 12850: 22.194\n","Training Accuracy: 82 \n","105/128\n","RunningLoss 12900: 22.564\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 12950: 22.164\n","Training Accuracy: 83 \n","107/128\n","RunningLoss 13000: 22.572\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 13050: 21.705\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 13100: 22.363\n","Training Accuracy: 85 \n","109/128\n","RunningLoss 13150: 22.629\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 13200: 22.606\n","Training Accuracy: 85 \n","109/128\n","RunningLoss 13250: 22.400\n","Training Accuracy: 85 \n","110/128\n","RunningLoss 13300: 22.916\n","Training Accuracy: 82 \n","105/128\n","RunningLoss 13350: 22.179\n","Training Accuracy: 85 \n","109/128\n","RunningLoss 13400: 22.592\n","Training Accuracy: 79 \n","102/128\n","RunningLoss 13450: 22.831\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 13500: 22.677\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 13550: 21.718\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 13600: 21.994\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 13650: 21.988\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 13700: 23.357\n","Training Accuracy: 88 \n","113/128\n","RunningLoss 13750: 22.080\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 13800: 22.424\n","Training Accuracy: 88 \n","113/128\n","RunningLoss 13850: 22.325\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 13900: 22.582\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 13950: 22.112\n","Training Accuracy: 88 \n","113/128\n","RunningLoss 14000: 22.142\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 14050: 22.176\n","epoch:  1\n","Training Accuracy: 85 \n","110/128\n","RunningLoss    50: 22.052\n","Training Accuracy: 85 \n","110/128\n","RunningLoss   100: 22.025\n","Training Accuracy: 89 \n","115/128\n","RunningLoss   150: 22.249\n","Training Accuracy: 85 \n","110/128\n","RunningLoss   200: 22.123\n","Training Accuracy: 88 \n","113/128\n","RunningLoss   250: 22.365\n","Training Accuracy: 85 \n","109/128\n","RunningLoss   300: 21.771\n","Training Accuracy: 83 \n","107/128\n","RunningLoss   350: 21.906\n","Training Accuracy: 86 \n","111/128\n","RunningLoss   400: 21.957\n","Training Accuracy: 86 \n","111/128\n","RunningLoss   450: 22.032\n","Training Accuracy: 87 \n","112/128\n","RunningLoss   500: 21.831\n","Training Accuracy: 92 \n","119/128\n","RunningLoss   550: 22.121\n","Training Accuracy: 92 \n","119/128\n","RunningLoss   600: 22.091\n","Training Accuracy: 82 \n","106/128\n","RunningLoss   650: 22.087\n","Training Accuracy: 75 \n","97/128\n","RunningLoss   700: 22.167\n","Training Accuracy: 88 \n","113/128\n","RunningLoss   750: 22.156\n","Training Accuracy: 91 \n","117/128\n","RunningLoss   800: 23.107\n","Training Accuracy: 90 \n","116/128\n","RunningLoss   850: 22.036\n","Training Accuracy: 85 \n","110/128\n","RunningLoss   900: 21.876\n","Training Accuracy: 85 \n","110/128\n","RunningLoss   950: 22.089\n","Training Accuracy: 85 \n","109/128\n","RunningLoss  1000: 22.007\n","Training Accuracy: 84 \n","108/128\n","RunningLoss  1050: 21.993\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  1100: 22.065\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  1150: 21.827\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  1200: 21.849\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  1250: 21.869\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  1300: 21.710\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  1350: 21.597\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  1400: 21.373\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  1450: 21.705\n","Training Accuracy: 85 \n","109/128\n","RunningLoss  1500: 21.541\n","Training Accuracy: 85 \n","109/128\n","RunningLoss  1550: 22.158\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  1600: 21.410\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  1650: 21.527\n","Training Accuracy: 85 \n","109/128\n","RunningLoss  1700: 21.133\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  1750: 21.999\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  1800: 21.673\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  1850: 21.561\n","Training Accuracy: 85 \n","109/128\n","RunningLoss  1900: 21.379\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  1950: 21.407\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  2000: 21.591\n","Training Accuracy: 84 \n","108/128\n","RunningLoss  2050: 21.559\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  2100: 22.130\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  2150: 22.096\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  2200: 21.765\n","Training Accuracy: 84 \n","108/128\n","RunningLoss  2250: 21.588\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  2300: 21.508\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  2350: 21.548\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  2400: 21.376\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  2450: 22.386\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  2500: 21.436\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  2550: 21.503\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  2600: 21.674\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  2650: 22.188\n","Training Accuracy: 95 \n","122/128\n","RunningLoss  2700: 21.109\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  2750: 21.463\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  2800: 21.301\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  2850: 21.633\n","Training Accuracy: 85 \n","109/128\n","RunningLoss  2900: 21.426\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  2950: 21.732\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  3000: 21.377\n","Training Accuracy: 83 \n","107/128\n","RunningLoss  3050: 22.068\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  3100: 21.398\n","Training Accuracy: 95 \n","122/128\n","RunningLoss  3150: 21.168\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  3200: 21.187\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  3250: 21.817\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  3300: 21.493\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  3350: 22.214\n","Training Accuracy: 84 \n","108/128\n","RunningLoss  3400: 22.289\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  3450: 21.892\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  3500: 21.076\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  3550: 21.432\n","Training Accuracy: 83 \n","107/128\n","RunningLoss  3600: 21.404\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  3650: 21.717\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  3700: 21.711\n","Training Accuracy: 83 \n","107/128\n","RunningLoss  3750: 21.900\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  3800: 21.487\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  3850: 21.224\n","Training Accuracy: 81 \n","104/128\n","RunningLoss  3900: 21.530\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  3950: 21.003\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  4000: 22.100\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  4050: 21.485\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  4100: 21.138\n","Training Accuracy: 83 \n","107/128\n","RunningLoss  4150: 21.587\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  4200: 21.848\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  4250: 21.463\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  4300: 21.276\n","Training Accuracy: 82 \n","105/128\n","RunningLoss  4350: 21.374\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  4400: 22.254\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  4450: 20.902\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  4500: 21.069\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  4550: 21.593\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  4600: 21.274\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  4650: 20.990\n","Training Accuracy: 94 \n","121/128\n","RunningLoss  4700: 21.480\n","Training Accuracy: 76 \n","98/128\n","RunningLoss  4750: 22.111\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  4800: 21.587\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  4850: 21.406\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  4900: 21.076\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  4950: 21.400\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  5000: 21.865\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  5050: 21.432\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  5100: 21.050\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  5150: 21.599\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  5200: 21.142\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  5250: 21.345\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  5300: 20.835\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  5350: 21.505\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  5400: 21.489\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  5450: 21.045\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  5500: 21.010\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  5550: 21.530\n","Training Accuracy: 85 \n","109/128\n","RunningLoss  5600: 21.549\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  5650: 21.359\n","Training Accuracy: 94 \n","121/128\n","RunningLoss  5700: 21.328\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  5750: 21.070\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  5800: 21.034\n","Training Accuracy: 79 \n","102/128\n","RunningLoss  5850: 21.203\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  5900: 21.554\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  5950: 21.101\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  6000: 21.414\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  6050: 21.092\n","Training Accuracy: 84 \n","108/128\n","RunningLoss  6100: 20.992\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  6150: 21.409\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  6200: 20.873\n","Training Accuracy: 94 \n","121/128\n","RunningLoss  6250: 21.708\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  6300: 21.493\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  6350: 20.951\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  6400: 21.063\n","Training Accuracy: 82 \n","105/128\n","RunningLoss  6450: 21.810\n","Training Accuracy: 84 \n","108/128\n","RunningLoss  6500: 21.520\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  6550: 20.901\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  6600: 21.043\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  6650: 20.873\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  6700: 21.379\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  6750: 21.049\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  6800: 20.639\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  6850: 21.001\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  6900: 20.819\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  6950: 20.889\n","Training Accuracy: 82 \n","106/128\n","RunningLoss  7000: 20.917\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  7050: 21.052\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  7100: 20.984\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  7150: 21.256\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  7200: 21.575\n","Training Accuracy: 84 \n","108/128\n","RunningLoss  7250: 21.227\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  7300: 21.023\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  7350: 20.899\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  7400: 20.553\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  7450: 20.911\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  7500: 20.580\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  7550: 21.023\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  7600: 20.948\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  7650: 21.209\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  7700: 21.042\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  7750: 21.458\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  7800: 21.236\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  7850: 20.997\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  7900: 20.182\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  7950: 20.674\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  8000: 20.729\n","Training Accuracy: 85 \n","109/128\n","RunningLoss  8050: 20.816\n","Training Accuracy: 84 \n","108/128\n","RunningLoss  8100: 21.365\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  8150: 21.319\n","Training Accuracy: 82 \n","105/128\n","RunningLoss  8200: 20.762\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  8250: 21.682\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  8300: 20.577\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  8350: 20.779\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  8400: 20.735\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  8450: 21.278\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  8500: 20.854\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  8550: 20.972\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  8600: 21.294\n","Training Accuracy: 80 \n","103/128\n","RunningLoss  8650: 21.178\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  8700: 20.996\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  8750: 20.918\n","Training Accuracy: 85 \n","109/128\n","RunningLoss  8800: 20.670\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  8850: 21.123\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  8900: 21.029\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  8950: 21.238\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  9000: 21.130\n","Training Accuracy: 84 \n","108/128\n","RunningLoss  9050: 21.436\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  9100: 20.534\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  9150: 20.909\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  9200: 21.408\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  9250: 20.487\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  9300: 21.463\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  9350: 20.687\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  9400: 21.095\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  9450: 20.625\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  9500: 20.569\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  9550: 20.801\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  9600: 20.867\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  9650: 20.641\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  9700: 20.830\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  9750: 20.634\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  9800: 20.611\n","Training Accuracy: 83 \n","107/128\n","RunningLoss  9850: 20.902\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  9900: 20.581\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  9950: 20.511\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 10000: 20.599\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 10050: 20.725\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 10100: 20.690\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 10150: 20.662\n","Training Accuracy: 85 \n","110/128\n","RunningLoss 10200: 20.724\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 10250: 20.818\n","Training Accuracy: 88 \n","113/128\n","RunningLoss 10300: 20.900\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 10350: 20.860\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 10400: 21.059\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 10450: 21.812\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 10500: 21.450\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 10550: 21.075\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 10600: 20.458\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 10650: 20.224\n","Training Accuracy: 93 \n","120/128\n","RunningLoss 10700: 20.549\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 10750: 20.570\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 10800: 20.302\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 10850: 20.027\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 10900: 20.239\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 10950: 20.938\n","Training Accuracy: 94 \n","121/128\n","RunningLoss 11000: 20.487\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 11050: 20.279\n","Training Accuracy: 88 \n","113/128\n","RunningLoss 11100: 20.463\n","Training Accuracy: 93 \n","120/128\n","RunningLoss 11150: 20.283\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 11200: 21.051\n","Training Accuracy: 88 \n","113/128\n","RunningLoss 11250: 20.546\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 11300: 20.465\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 11350: 20.549\n","Training Accuracy: 93 \n","120/128\n","RunningLoss 11400: 20.886\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 11450: 21.541\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 11500: 20.219\n","Training Accuracy: 80 \n","103/128\n","RunningLoss 11550: 20.676\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 11600: 20.546\n","Training Accuracy: 93 \n","120/128\n","RunningLoss 11650: 20.483\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 11700: 20.220\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 11750: 20.270\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 11800: 20.349\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 11850: 20.982\n","Training Accuracy: 83 \n","107/128\n","RunningLoss 11900: 20.510\n","Training Accuracy: 82 \n","105/128\n","RunningLoss 11950: 21.336\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 12000: 20.417\n","Training Accuracy: 85 \n","110/128\n","RunningLoss 12050: 20.693\n","Training Accuracy: 88 \n","113/128\n","RunningLoss 12100: 20.423\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 12150: 20.779\n","Training Accuracy: 88 \n","113/128\n","RunningLoss 12200: 20.766\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 12250: 20.597\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 12300: 20.294\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 12350: 19.841\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 12400: 20.287\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 12450: 20.217\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 12500: 20.723\n","Training Accuracy: 95 \n","122/128\n","RunningLoss 12550: 20.772\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 12600: 20.301\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 12650: 20.599\n","Training Accuracy: 88 \n","113/128\n","RunningLoss 12700: 20.676\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 12750: 20.193\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 12800: 20.206\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 12850: 21.044\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 12900: 20.139\n","Training Accuracy: 93 \n","120/128\n","RunningLoss 12950: 20.727\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 13000: 20.243\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 13050: 20.028\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 13100: 20.430\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 13150: 20.107\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 13200: 19.892\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 13250: 20.168\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 13300: 19.934\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 13350: 20.816\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 13400: 20.946\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 13450: 20.223\n","Training Accuracy: 88 \n","113/128\n","RunningLoss 13500: 20.024\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 13550: 20.293\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 13600: 20.518\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 13650: 20.037\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 13700: 20.241\n","Training Accuracy: 83 \n","107/128\n","RunningLoss 13750: 20.524\n","Training Accuracy: 93 \n","120/128\n","RunningLoss 13800: 20.198\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 13850: 20.197\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 13900: 20.231\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 13950: 20.555\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 14000: 20.274\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 14050: 20.280\n","epoch:  2\n","Training Accuracy: 87 \n","112/128\n","RunningLoss    50: 20.261\n","Training Accuracy: 85 \n","110/128\n","RunningLoss   100: 19.838\n","Training Accuracy: 92 \n","119/128\n","RunningLoss   150: 19.833\n","Training Accuracy: 89 \n","114/128\n","RunningLoss   200: 20.471\n","Training Accuracy: 92 \n","118/128\n","RunningLoss   250: 20.403\n","Training Accuracy: 96 \n","123/128\n","RunningLoss   300: 20.003\n","Training Accuracy: 89 \n","115/128\n","RunningLoss   350: 20.419\n","Training Accuracy: 89 \n","114/128\n","RunningLoss   400: 20.440\n","Training Accuracy: 95 \n","122/128\n","RunningLoss   450: 20.422\n","Training Accuracy: 82 \n","106/128\n","RunningLoss   500: 20.467\n","Training Accuracy: 87 \n","112/128\n","RunningLoss   550: 20.088\n","Training Accuracy: 93 \n","120/128\n","RunningLoss   600: 19.750\n","Training Accuracy: 91 \n","117/128\n","RunningLoss   650: 19.995\n","Training Accuracy: 90 \n","116/128\n","RunningLoss   700: 20.304\n","Training Accuracy: 87 \n","112/128\n","RunningLoss   750: 19.554\n","Training Accuracy: 90 \n","116/128\n","RunningLoss   800: 20.007\n","Training Accuracy: 92 \n","118/128\n","RunningLoss   850: 20.454\n","Training Accuracy: 94 \n","121/128\n","RunningLoss   900: 19.752\n","Training Accuracy: 92 \n","119/128\n","RunningLoss   950: 20.640\n","Training Accuracy: 95 \n","122/128\n","RunningLoss  1000: 20.140\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  1050: 20.085\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  1100: 20.118\n","Training Accuracy: 80 \n","103/128\n","RunningLoss  1150: 20.264\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  1200: 20.395\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  1250: 20.355\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  1300: 19.909\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  1350: 20.220\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  1400: 19.960\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  1450: 19.856\n","Training Accuracy: 95 \n","122/128\n","RunningLoss  1500: 20.356\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  1550: 20.353\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  1600: 20.075\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  1650: 19.963\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  1700: 19.863\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  1750: 20.026\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  1800: 20.235\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  1850: 20.302\n","Training Accuracy: 85 \n","109/128\n","RunningLoss  1900: 20.017\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  1950: 19.864\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  2000: 19.768\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  2050: 19.840\n","Training Accuracy: 95 \n","122/128\n","RunningLoss  2100: 20.847\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  2150: 20.337\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  2200: 20.335\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  2250: 19.821\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  2300: 20.090\n","Training Accuracy: 85 \n","109/128\n","RunningLoss  2350: 19.856\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  2400: 19.997\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  2450: 19.931\n","Training Accuracy: 94 \n","121/128\n","RunningLoss  2500: 19.837\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  2550: 20.366\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  2600: 20.080\n","Training Accuracy: 94 \n","121/128\n","RunningLoss  2650: 19.617\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  2700: 19.943\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  2750: 19.707\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  2800: 19.832\n","Training Accuracy: 94 \n","121/128\n","RunningLoss  2850: 19.510\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  2900: 20.329\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  2950: 20.186\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  3000: 20.026\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  3050: 20.045\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  3100: 19.825\n","Training Accuracy: 82 \n","106/128\n","RunningLoss  3150: 19.668\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  3200: 19.894\n","Training Accuracy: 84 \n","108/128\n","RunningLoss  3250: 20.385\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  3300: 20.132\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  3350: 19.706\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  3400: 19.954\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  3450: 20.120\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  3500: 19.865\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  3550: 19.670\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  3600: 20.253\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  3650: 19.938\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  3700: 19.885\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  3750: 19.931\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  3800: 19.729\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  3850: 20.432\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  3900: 19.769\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  3950: 19.972\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  4000: 19.852\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  4050: 19.766\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  4100: 19.933\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  4150: 19.674\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  4200: 19.842\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  4250: 19.969\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  4300: 20.132\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  4350: 20.055\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  4400: 20.217\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  4450: 20.267\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  4500: 19.602\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  4550: 19.900\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  4600: 19.534\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  4650: 19.647\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  4700: 19.927\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  4750: 19.912\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  4800: 20.929\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  4850: 19.777\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  4900: 19.877\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  4950: 19.647\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  5000: 19.941\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  5050: 19.770\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  5100: 19.750\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  5150: 19.737\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  5200: 19.783\n","Training Accuracy: 94 \n","121/128\n","RunningLoss  5250: 19.569\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  5300: 19.648\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  5350: 19.755\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  5400: 19.417\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  5450: 19.778\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  5500: 19.523\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  5550: 19.440\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  5600: 19.760\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  5650: 19.656\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  5700: 20.018\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  5750: 19.871\n","Training Accuracy: 94 \n","121/128\n","RunningLoss  5800: 19.726\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  5850: 19.633\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  5900: 19.648\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  5950: 19.745\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  6000: 19.736\n","Training Accuracy: 96 \n","124/128\n","RunningLoss  6050: 19.440\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  6100: 19.884\n","Training Accuracy: 95 \n","122/128\n","RunningLoss  6150: 19.884\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  6200: 19.865\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  6250: 19.709\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  6300: 19.698\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  6350: 19.839\n","Training Accuracy: 85 \n","109/128\n","RunningLoss  6400: 19.795\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  6450: 19.693\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  6500: 19.482\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  6550: 19.536\n","Training Accuracy: 94 \n","121/128\n","RunningLoss  6600: 19.392\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  6650: 20.062\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  6700: 19.765\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  6750: 19.587\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  6800: 19.900\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  6850: 19.660\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  6900: 19.655\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  6950: 19.587\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  7000: 19.800\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  7050: 19.728\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  7100: 19.532\n","Training Accuracy: 94 \n","121/128\n","RunningLoss  7150: 19.740\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  7200: 19.613\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  7250: 19.612\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  7300: 19.706\n","Training Accuracy: 94 \n","121/128\n","RunningLoss  7350: 19.977\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  7400: 19.937\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  7450: 19.697\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  7500: 20.138\n","Training Accuracy: 96 \n","123/128\n","RunningLoss  7550: 19.580\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  7600: 19.462\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  7650: 19.697\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  7700: 19.587\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  7750: 20.387\n","Training Accuracy: 96 \n","124/128\n","RunningLoss  7800: 19.622\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  7850: 19.525\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  7900: 19.594\n","Training Accuracy: 86 \n","111/128\n","RunningLoss  7950: 19.445\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  8000: 19.596\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  8050: 19.561\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  8100: 19.657\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  8150: 19.837\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  8200: 19.267\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  8250: 19.826\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  8300: 19.546\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  8350: 20.050\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  8400: 19.693\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  8450: 19.715\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  8500: 19.327\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  8550: 19.581\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  8600: 19.517\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  8650: 19.626\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  8700: 19.841\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  8750: 19.931\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  8800: 19.336\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  8850: 19.523\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  8900: 19.547\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  8950: 19.438\n","Training Accuracy: 94 \n","121/128\n","RunningLoss  9000: 19.659\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  9050: 19.585\n","Training Accuracy: 84 \n","108/128\n","RunningLoss  9100: 19.755\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  9150: 19.684\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  9200: 19.751\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  9250: 19.720\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  9300: 19.593\n","Training Accuracy: 88 \n","113/128\n","RunningLoss  9350: 19.381\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  9400: 19.388\n","Training Accuracy: 85 \n","110/128\n","RunningLoss  9450: 19.631\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  9500: 19.804\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  9550: 19.298\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  9600: 19.415\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  9650: 19.493\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  9700: 19.992\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  9750: 19.231\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  9800: 19.734\n","Training Accuracy: 91 \n","117/128\n","RunningLoss  9850: 19.617\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  9900: 19.549\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  9950: 19.619\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 10000: 19.708\n","Training Accuracy: 95 \n","122/128\n","RunningLoss 10050: 19.672\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 10100: 19.464\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 10150: 19.611\n","Training Accuracy: 93 \n","120/128\n","RunningLoss 10200: 19.426\n","Training Accuracy: 94 \n","121/128\n","RunningLoss 10250: 19.338\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 10300: 19.360\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 10350: 19.304\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 10400: 19.504\n","Training Accuracy: 95 \n","122/128\n","RunningLoss 10450: 19.142\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 10500: 19.662\n","Training Accuracy: 87 \n","112/128\n","RunningLoss 10550: 19.562\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 10600: 19.418\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 10650: 19.299\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 10700: 19.412\n","Training Accuracy: 95 \n","122/128\n","RunningLoss 10750: 19.510\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 10800: 19.581\n","Training Accuracy: 88 \n","113/128\n","RunningLoss 10850: 19.356\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 10900: 19.475\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 10950: 20.000\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 11000: 19.638\n","Training Accuracy: 96 \n","123/128\n","RunningLoss 11050: 19.399\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 11100: 19.667\n","Training Accuracy: 88 \n","113/128\n","RunningLoss 11150: 19.827\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 11200: 19.757\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 11250: 19.704\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 11300: 19.628\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 11350: 19.306\n","Training Accuracy: 96 \n","124/128\n","RunningLoss 11400: 19.436\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 11450: 19.691\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 11500: 19.231\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 11550: 19.807\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 11600: 19.448\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 11650: 19.885\n","Training Accuracy: 88 \n","113/128\n","RunningLoss 11700: 19.565\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 11750: 20.052\n","Training Accuracy: 93 \n","120/128\n","RunningLoss 11800: 19.749\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 11850: 19.054\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 11900: 19.523\n","Training Accuracy: 88 \n","113/128\n","RunningLoss 11950: 19.435\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 12000: 19.862\n","Training Accuracy: 93 \n","120/128\n","RunningLoss 12050: 19.682\n","Training Accuracy: 95 \n","122/128\n","RunningLoss 12100: 19.152\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 12150: 19.538\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 12200: 19.475\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 12250: 19.368\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 12300: 19.233\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 12350: 19.004\n","Training Accuracy: 83 \n","107/128\n","RunningLoss 12400: 19.935\n","Training Accuracy: 85 \n","110/128\n","RunningLoss 12450: 19.395\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 12500: 19.459\n","Training Accuracy: 94 \n","121/128\n","RunningLoss 12550: 19.044\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 12600: 19.245\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 12650: 19.348\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 12700: 19.556\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 12750: 19.510\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 12800: 19.257\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 12850: 19.216\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 12900: 19.382\n","Training Accuracy: 94 \n","121/128\n","RunningLoss 12950: 19.325\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 13000: 19.347\n","Training Accuracy: 89 \n","114/128\n","RunningLoss 13050: 19.265\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 13100: 19.589\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 13150: 19.515\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 13200: 19.555\n","Training Accuracy: 86 \n","111/128\n","RunningLoss 13250: 19.394\n","Training Accuracy: 93 \n","120/128\n","RunningLoss 13300: 19.150\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 13350: 19.550\n","Training Accuracy: 95 \n","122/128\n","RunningLoss 13400: 19.343\n","Training Accuracy: 96 \n","123/128\n","RunningLoss 13450: 19.606\n","Training Accuracy: 89 \n","115/128\n","RunningLoss 13500: 19.277\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 13550: 19.430\n","Training Accuracy: 93 \n","120/128\n","RunningLoss 13600: 19.350\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 13650: 19.273\n","Training Accuracy: 94 \n","121/128\n","RunningLoss 13700: 19.644\n","Training Accuracy: 91 \n","117/128\n","RunningLoss 13750: 19.379\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 13800: 19.308\n","Training Accuracy: 95 \n","122/128\n","RunningLoss 13850: 19.333\n","Training Accuracy: 92 \n","119/128\n","RunningLoss 13900: 19.696\n","Training Accuracy: 95 \n","122/128\n","RunningLoss 13950: 19.346\n","Training Accuracy: 90 \n","116/128\n","RunningLoss 14000: 19.264\n","Training Accuracy: 92 \n","118/128\n","RunningLoss 14050: 19.065\n","epoch:  3\n","Training Accuracy: 91 \n","117/128\n","RunningLoss    50: 18.869\n","Training Accuracy: 92 \n","119/128\n","RunningLoss   100: 19.455\n","Training Accuracy: 96 \n","123/128\n","RunningLoss   150: 19.107\n","Training Accuracy: 88 \n","113/128\n","RunningLoss   200: 19.371\n","Training Accuracy: 89 \n","114/128\n","RunningLoss   250: 19.399\n","Training Accuracy: 89 \n","114/128\n","RunningLoss   300: 19.215\n","Training Accuracy: 92 \n","118/128\n","RunningLoss   350: 19.148\n","Training Accuracy: 91 \n","117/128\n","RunningLoss   400: 19.156\n","Training Accuracy: 89 \n","114/128\n","RunningLoss   450: 19.229\n","Training Accuracy: 96 \n","123/128\n","RunningLoss   500: 19.234\n","Training Accuracy: 89 \n","115/128\n","RunningLoss   550: 18.941\n","Training Accuracy: 96 \n","123/128\n","RunningLoss   600: 19.133\n","Training Accuracy: 96 \n","123/128\n","RunningLoss   650: 19.176\n","Training Accuracy: 95 \n","122/128\n","RunningLoss   700: 19.151\n","Training Accuracy: 92 \n","119/128\n","RunningLoss   750: 19.090\n","Training Accuracy: 95 \n","122/128\n","RunningLoss   800: 19.318\n","Training Accuracy: 95 \n","122/128\n","RunningLoss   850: 19.103\n","Training Accuracy: 94 \n","121/128\n","RunningLoss   900: 19.256\n","Training Accuracy: 94 \n","121/128\n","RunningLoss   950: 19.121\n","Training Accuracy: 87 \n","112/128\n","RunningLoss  1000: 19.355\n","Training Accuracy: 96 \n","124/128\n","RunningLoss  1050: 19.027\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  1100: 19.058\n","Training Accuracy: 96 \n","123/128\n","RunningLoss  1150: 19.226\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  1200: 19.135\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  1250: 19.159\n","Training Accuracy: 90 \n","116/128\n","RunningLoss  1300: 19.156\n","Training Accuracy: 89 \n","114/128\n","RunningLoss  1350: 19.169\n","Training Accuracy: 92 \n","118/128\n","RunningLoss  1400: 19.122\n","Training Accuracy: 93 \n","120/128\n","RunningLoss  1450: 19.129\n","Training Accuracy: 92 \n","119/128\n","RunningLoss  1500: 19.103\n","Training Accuracy: 89 \n","115/128\n","RunningLoss  1550: 19.053\n"],"name":"stdout"}]}]}